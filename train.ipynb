{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8aa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08debb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModel\n",
    "from model.layer import CharCNN, FeatureEmbedding\n",
    "from model.layer.utils import get_extended_attention_mask\n",
    "from model.layer.featureEmbed import FeatureRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04885b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_layers = nn.ModuleList([AutoModel.from_pretrained('vinai/phobert-base').base_model.encoder.layer[i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79123162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum([i.numel() for i in bert_layers.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27de615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_embed = AutoModel.from_pretrained('vinai/phobert-base').base_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9020c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum([i.numel() for i in bert_embed.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28189d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf518f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     BertTop(top=2),\n",
    "#     nn.LSTM(768, 768//2, batch_first=True, num_layers=2,bidirectional=True),\n",
    "    \n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714a9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.static_features import FeatureExtractor, Feature,NERdataset,logger, init_logger\n",
    "from preprocess.processcer_join_bert import NERProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b663f89",
   "metadata": {},
   "outputs": [],
   "source": [
    " feats=Feature(\"/home/tuenguyen/Desktop/24mar2021/task_nlp/join_task_gender_department/resources/features/feature_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe5100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9d8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.word_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81189124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.word_net.feature_embeddings\n",
    "# a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4314d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "processor = NERProcessor(\"./dataset\", tokenizer)\n",
    "processor.labels=[\"O\",'B-GENDER','I-GENDER','B-LOC','I-LOC']\n",
    "processor.label_map= {label: i for i, label in enumerate(processor.labels, 1)}\n",
    "a = processor.get_example(\"train\",use_feats=True)\n",
    "\n",
    "# features = processor.convert_examples_to_features(a, 126,feats)\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9e2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "def build_dataset(args, processor, data_type='train', feature=None, device=torch.device('cpu')):\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args['data_dir'], 'cached_{}_{}_{}'.format(\n",
    "        data_type,\n",
    "        list(filter(None, args['model_name_or_path'].split('/'))).pop(),\n",
    "        str(args['max_seq_length'])))\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        print(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        print(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "        examples = processor.get_example(data_type, feature is not None)\n",
    "        features = processor.convert_examples_to_features(examples, args['max_seq_length'], feature)\n",
    "        print(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features[:-1], cached_features_file)\n",
    "    return NERdataset(features, device)\n",
    "\n",
    "\n",
    "\n",
    "def caculator_metric(preds, golds, labels):\n",
    "#     pred_iob_labels = [labels[label_id] for label_id in preds]\n",
    "#     gold_iob_labels = [labels[label_id] for label_id in golds]\n",
    "\n",
    "#     pred_labels = [labels[label_id - 1].split(\"-\")[-1].strip() for label_id in preds]\n",
    "#     gold_labels = [labels[label_id - 1].split(\"-\")[-1].strip() for label_id in golds]\n",
    "\n",
    "#     iob_metric = classification_report(pred_iob_labels, gold_iob_labels, output_dict=True)\n",
    "    metric = classification_report(preds, golds, output_dict=True)\n",
    "\n",
    "    return metric, metric\n",
    "\n",
    "\n",
    "def update_model_weights(model, iterator, optimizer, scheduler):\n",
    "    # init static variables\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        optimizer.zero_grad()\n",
    "        tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = batch\n",
    "        loss, _ = model.calculate_loss(token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks,\n",
    "                                       feats)\n",
    "        tr_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return tr_loss\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, label_map):\n",
    "    # init static variables\n",
    "    preds = []\n",
    "    golds = []\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = batch\n",
    "        loss, (logits, labels) = model.calculate_loss(token_ids, attention_masks, token_mask, segment_ids, label_ids,\n",
    "                                                      label_masks, feats)\n",
    "        eval_loss += loss.item()\n",
    "        logits = torch.argmax(nn.functional.softmax(logits, dim=-1), dim=-1)\n",
    "        pred = logits.detach().cpu().numpy()\n",
    "        gold = labels.to('cpu').numpy()\n",
    "        preds.extend(pred)\n",
    "        golds.extend(gold)\n",
    "#     print(preds,golds)\n",
    "    preds = np.concatenate(preds)\n",
    "    golds=np.concatenate(golds)\n",
    "    iob_metric, metric = caculator_metric(preds, golds, label_map)\n",
    "\n",
    "    return eval_loss, iob_metric, metric\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d5a3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.randn(32,)\n",
    "b=a\n",
    "np.concatenate([a,b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb70387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Build data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s ./dataset/cached_train_phobert_100\n",
      "Loading features from cached file %s ./dataset/cached_test_phobert_100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(23072021)\n",
    "np.random.seed(23072021)\n",
    "torch.manual_seed(23072021)\n",
    "summary_writer = SummaryWriter('./logs')\n",
    "init_logger(f\"./logs/vner_trainning.log\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "processor = NERProcessor(\"./dataset\", tokenizer)\n",
    "processor.labels=[\"O\",'B-GENDER','I-GENDER','B-LOC','I-LOC']\n",
    "processor.label_map= {label: i for i, label in enumerate(processor.labels, 1)}\n",
    "num_labels = processor.get_num_labels()\n",
    "logger.info(\"Build data ...\")\n",
    "args={\n",
    "    'model_name_or_path':'phobert',\n",
    "    'data_dir':'./dataset',\n",
    "    'max_seq_length':100\n",
    "}\n",
    "train_data = build_dataset(args, processor, data_type='train', feature=feats, device=device)\n",
    "eval_data = build_dataset(args, processor, data_type='test', feature=feats, device=device)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_iterator = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_iterator = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50af53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef5e266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,  6207, 20827,  ...,     0,     0,     0],\n",
       "        [    3,   168,  9073,  ...,     0,     0,     0],\n",
       "        [    3,  2080,    33,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    3,  1144,   343,  ...,     0,     0,     0],\n",
       "        [    3,   793,  2900,  ...,     0,     0,     0],\n",
       "        [    3,  5217,    41,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60cb44ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2],a[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "473aa143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.tokens, token_id_tensors, attention_mask_tensors, token_mask_tensors, segment_id_tensors, \\\n",
    "#                label_id_tensors, label_mask_tensors, feat_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846a774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids, attention_masks, token_masks, segment_ids, label_ids, label_masks, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf6d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import BertCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e4b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertCRF(n_layer_bert=2)\n",
    "model = model.to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"/home/tuenguyen/Desktop/24mar2021/task_nlp/join_task_gender_department/checkpoints/vner_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ff82a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==============================Epoch 0==============================\n",
      "Iteration: 100%|██████████| 125/125 [02:19<00:00,  1.12s/it]\n",
      "train Loss: 9.353205114603043\n",
      "Iteration: 100%|██████████| 70/70 [00:15<00:00,  4.66it/s]\n",
      "eval Loss: 87.12612441182137\n",
      "F1-Score tag: 0.8740105381527828\n",
      "F1-Score IOB-tag: 0.8740105381527828\n",
      "Metric:\n",
      "\tO: 0.9966328974028366\n",
      "\tB-GENDER: 0.9370134465675866 \n",
      "\tI-GENDER: 0.5\n",
      "\tB-LOC: 0.8931419457735247\n",
      "\tI-LOC: 0.9172749391727495\n",
      "Model save at epoch 0 with best score 0.8740105381527828\n",
      "==============================Epoch 1==============================\n",
      "Iteration: 100%|██████████| 125/125 [02:12<00:00,  1.06s/it]\n",
      "train Loss: 4.99702849262394\n",
      "Iteration: 100%|██████████| 70/70 [00:14<00:00,  4.77it/s]\n",
      "eval Loss: 99.12007230520248\n",
      "F1-Score tag: 0.8735246318054665\n",
      "F1-Score IOB-tag: 0.8735246318054665\n",
      "Metric:\n",
      "\tO: 0.996729329954623\n",
      "\tB-GENDER: 0.9405940594059405 \n",
      "\tI-GENDER: 0.4878048780487805\n",
      "\tB-LOC: 0.8951386098834874\n",
      "\tI-LOC: 0.9208809135399674\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "best_score = -1\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 1e-4},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "num_train_optimization_steps = len(train_iterator) // 1 * 100\n",
    "warmup_steps = int(0.1 * num_train_optimization_steps)\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=3e-4,)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=num_train_optimization_steps)\n",
    "for e in range(2):\n",
    "    logger.info(\"=\"*30 + f\"Epoch {e}\" + \"=\"*30)\n",
    "    tr_loss = update_model_weights(model, train_iterator, optimizer, scheduler)\n",
    "    logger.info(f\"train Loss: {tr_loss}\")\n",
    "    eval_loss, iob_metric, metric = evaluate(model, eval_iterator, processor.labels)\n",
    "    logger.info(f\"eval Loss: {eval_loss}\")\n",
    "    logger.info(f\"F1-Score tag: {metric['macro avg']['f1-score']}\")\n",
    "    logger.info(f\"F1-Score IOB-tag: {iob_metric['macro avg']['f1-score']}\")\n",
    "    logger.info(f\"Metric:\")\n",
    "#     print(metric)\n",
    "    logger.info(f\"\\tO: {metric['1']['f1-score']}\")\n",
    "    logger.info(f\"\\tB-GENDER: {metric['2']['f1-score']} \")\n",
    "    logger.info(f\"\\tI-GENDER: {metric['3']['f1-score']}\")\n",
    "    logger.info(f\"\\tB-LOC: {metric['4']['f1-score']}\")\n",
    "    logger.info(f\"\\tI-LOC: {metric['5']['f1-score']}\")\n",
    "\n",
    "\n",
    "    if iob_metric['macro avg']['f1-score'] > best_score:\n",
    "        best_score = iob_metric['macro avg']['f1-score']\n",
    "        best_epoch = e\n",
    "        model_path = f\"checkpoints/vner_model.bin\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logger.info(f\"Model save at epoch {best_epoch} with best score {best_score}\")\n",
    "        summary_writer.add_text(\"Best result\", f\"F1-Score: {best_score}\", best_epoch)\n",
    "        summary_writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "877d8f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-GENDER', 'I-GENDER', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " processor.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3556c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'em muốn đặt khám cho mẹ em thái tự mẫu'\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fdb3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 860.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cho', 'O'), ('anh', 'B-GENDER'), ('nguyễn_văn', 'O'), ('nam', 'O'), ('~', 'O'), ('xin', 'O'), ('thông_tin', 'O'), ('khám', 'O'), ('bệnh', 'O'), ('ở', 'O'), ('khoa', 'B-LOC'), ('cấp_cứu', 'I-LOC'), ('phía', 'O'), ('miền', 'O'), ('nam', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'cho anh nguyễn   văn nam ~  xin thông tin khám bệnh ở khoa cấp cứu phía miền nam'\n",
    "predict, sentence=model.predict(text,device='cuda')\n",
    "print(list(zip(sentence.split(),predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee933c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence),len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29025989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence= sentence.split()\n",
    "entities = predict\n",
    "entities_final = []\n",
    "step = 0\n",
    "while step < len(sentence):\n",
    "    if entities[step] == 'O':\n",
    "        entities_final.append([entities[step],sentence[step]])\n",
    "        step += 1\n",
    "    elif 'B' in entities[step]:\n",
    "        a = entities[step].split(\"-\")[1]\n",
    "        entities_final.append([a, \"\"])\n",
    "        while step < len(sentence) and \"-\" in entities[step] and entities[step].split(\"-\")[1] == a:\n",
    "            entities_final[-1][1] = entities_final[-1][1] + \" \" + sentence[step]\n",
    "            step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fce2856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'cho'],\n",
       " ['GENDER', ' anh'],\n",
       " ['O', 'nguyễn_văn'],\n",
       " ['O', 'nam'],\n",
       " ['O', '~'],\n",
       " ['O', 'xin'],\n",
       " ['O', 'thông_tin'],\n",
       " ['O', 'khám'],\n",
       " ['O', 'bệnh'],\n",
       " ['O', 'ở'],\n",
       " ['LOC', ' khoa cấp_cứu'],\n",
       " ['O', 'phía'],\n",
       " ['O', 'miền'],\n",
       " ['O', 'nam']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e934046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [{'start': 32, 'end': 40, 'value': 'department', 'extractor': 'template_matching_department', 'confidence': 1.0}]}\n",
      "tim mạch\n"
     ]
    }
   ],
   "source": [
    "from preprocess.matcher_lib import GenderMatcher, DepartmentMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e2cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DepartmentMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2957a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n",
      "{'entities': []}\n"
     ]
    }
   ],
   "source": [
    "for a in entities_final:\n",
    "    print(gen.match(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = dataset_test[0]\n",
    "feats = {k:v[None,...] for k,v in feats.items()}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(token_ids[None,...], attention_masks[None,...], feats)\n",
    "    out1 = model.crf.decode(out)\n",
    "    out2 = torch.argmax(out,axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c342297",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2[token_mask==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb429cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mask==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ee61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c418cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = [max(i-1,0) for i in out1[0]]\n",
    "# out = out - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.label_map\n",
    "# print([processor.labels[i] for i in out1] )\n",
    "print([(ix,processor.labels[max(i-1,0)]) for i,ix in zip(out[0][1:],text.split())] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slot_preds = np.array(model.crf.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([processor.labels[i] for i in out1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[132][0],train_data[132][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(eval_data[1000][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'cho anh nguyễn   văn nam ~  xin thông tin khám bệnh ở khoa cấp cứu phía miền nam'\n",
    "from underthesea import word_tokenize\n",
    "text=word_tokenize(text,format='text')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3c5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e00dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
