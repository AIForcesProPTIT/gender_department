{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8aa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08debb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModel\n",
    "from model.layer import CharCNN, FeatureEmbedding\n",
    "from model.layer.utils import get_extended_attention_mask\n",
    "from model.layer.featureEmbed import FeatureRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04885b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_layers = nn.ModuleList([AutoModel.from_pretrained('vinai/phobert-base').base_model.encoder.layer[i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79123162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum([i.numel() for i in bert_layers.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27de615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_embed = AutoModel.from_pretrained('vinai/phobert-base').base_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9020c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum([i.numel() for i in bert_embed.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28189d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf518f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     BertTop(top=2),\n",
    "#     nn.LSTM(768, 768//2, batch_first=True, num_layers=2,bidirectional=True),\n",
    "    \n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714a9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.static_features import FeatureExtractor, Feature,NERdataset,logger, init_logger\n",
    "from preprocess.processcer_join_bert import NERProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b663f89",
   "metadata": {},
   "outputs": [],
   "source": [
    " feats=Feature(\"/home/tuenguyen/Desktop/24mar2021/task_nlp/join_task_gender_department/resources/features/feature_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe5100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9d8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.word_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81189124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.word_net.feature_embeddings\n",
    "# a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4314d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "processor = NERProcessor(\"./dataset\", tokenizer)\n",
    "processor.labels=[\"O\",'B-GENDER','I-GENDER','B-LOC','I-LOC']\n",
    "processor.label_map= {label: i for i, label in enumerate(processor.labels, 1)}\n",
    "a = processor.get_example(\"train\",use_feats=True)\n",
    "\n",
    "# features = processor.convert_examples_to_features(a, 126,feats)\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9e2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "def build_dataset(args, processor, data_type='train', feature=None, device=torch.device('cpu')):\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args['data_dir'], 'cached_{}_{}_{}'.format(\n",
    "        data_type,\n",
    "        list(filter(None, args['model_name_or_path'].split('/'))).pop(),\n",
    "        str(args['max_seq_length'])))\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        print(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        print(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "        examples = processor.get_example(data_type, feature is not None)\n",
    "        features = processor.convert_examples_to_features(examples, args['max_seq_length'], feature)\n",
    "        print(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features[:-1], cached_features_file)\n",
    "    return NERdataset(features, device)\n",
    "\n",
    "\n",
    "\n",
    "def caculator_metric(preds, golds, labels):\n",
    "#     pred_iob_labels = [labels[label_id] for label_id in preds]\n",
    "#     gold_iob_labels = [labels[label_id] for label_id in golds]\n",
    "\n",
    "#     pred_labels = [labels[label_id - 1].split(\"-\")[-1].strip() for label_id in preds]\n",
    "#     gold_labels = [labels[label_id - 1].split(\"-\")[-1].strip() for label_id in golds]\n",
    "\n",
    "#     iob_metric = classification_report(pred_iob_labels, gold_iob_labels, output_dict=True)\n",
    "    metric = classification_report(preds, golds, output_dict=True)\n",
    "\n",
    "    return metric, metric\n",
    "\n",
    "\n",
    "def update_model_weights(model, iterator, optimizer, scheduler):\n",
    "    # init static variables\n",
    "    tr_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        optimizer.zero_grad()\n",
    "        tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = batch\n",
    "        loss, _ = model.calculate_loss(token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks,\n",
    "                                       feats)\n",
    "        tr_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return tr_loss\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, label_map):\n",
    "    # init static variables\n",
    "    preds = []\n",
    "    golds = []\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = batch\n",
    "        loss, (logits, labels) = model.calculate_loss(token_ids, attention_masks, token_mask, segment_ids, label_ids,\n",
    "                                                      label_masks, feats)\n",
    "        eval_loss += loss.item()\n",
    "        logits = torch.argmax(nn.functional.softmax(logits, dim=-1), dim=-1)\n",
    "        pred = logits.detach().cpu().numpy()\n",
    "        gold = labels.to('cpu').numpy()\n",
    "        preds.extend(pred)\n",
    "        golds.extend(gold)\n",
    "#     print(preds,golds)\n",
    "    preds = np.concatenate(preds)\n",
    "    golds=np.concatenate(golds)\n",
    "    iob_metric, metric = caculator_metric(preds, golds, label_map)\n",
    "\n",
    "    return eval_loss, iob_metric, metric\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d5a3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.randn(32,)\n",
    "b=a\n",
    "np.concatenate([a,b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb70387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Build data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file %s ./dataset/cached_train_phobert_100\n",
      "Loading features from cached file %s ./dataset/cached_test_phobert_100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(23072021)\n",
    "np.random.seed(23072021)\n",
    "torch.manual_seed(23072021)\n",
    "summary_writer = SummaryWriter('./logs')\n",
    "init_logger(f\"./logs/vner_trainning.log\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "processor = NERProcessor(\"./dataset\", tokenizer)\n",
    "processor.labels=[\"O\",'B-GENDER','I-GENDER','B-LOC','I-LOC']\n",
    "processor.label_map= {label: i for i, label in enumerate(processor.labels, 1)}\n",
    "num_labels = processor.get_num_labels()\n",
    "logger.info(\"Build data ...\")\n",
    "args={\n",
    "    'model_name_or_path':'phobert',\n",
    "    'data_dir':'./dataset',\n",
    "    'max_seq_length':100\n",
    "}\n",
    "train_data = build_dataset(args, processor, data_type='train', feature=feats, device=device)\n",
    "eval_data = build_dataset(args, processor, data_type='test', feature=feats, device=device)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_iterator = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n",
    "\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_iterator = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50af53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473aa143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.tokens, token_id_tensors, attention_mask_tensors, token_mask_tensors, segment_id_tensors, \\\n",
    "#                label_id_tensors, label_mask_tensors, feat_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846a774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids, attention_masks, token_masks, segment_ids, label_ids, label_masks, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf6d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.layer.crf import CRF\n",
    "class BertTop(nn.Module):\n",
    "    def __init__(self, top, feat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_net  = FeatureRep(feat)\n",
    "        \n",
    "        b = AutoModel.from_pretrained('vinai/phobert-base').to(\"cpu\")\n",
    "        \n",
    "        self.bert_embed =b.base_model.embeddings \n",
    "        \n",
    "        self.bert_layers = nn.ModuleList([b.base_model.encoder.layer[i] for i in range(top)])\n",
    "        \n",
    "        self.lstm = nn.LSTM(768 + 23, 768//2, batch_first=True, num_layers=2,bidirectional=True)\n",
    "        \n",
    "        self.crf = CRF(num_tags=6, batch_first=True)\n",
    "        \n",
    "        self.slot_classifier  = nn.Sequential(\n",
    "            \n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(768, 6)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask,  feat_rep):\n",
    "        \n",
    "        bert_output = self.bert_embed(input_ids)\n",
    "        \n",
    "        attention_mask = get_extended_attention_mask(attention_mask,input_ids.shape,input_ids.device)\n",
    "        \n",
    "        outputs = self.word_net(feat_rep)\n",
    "\n",
    "        for layer in self.bert_layers:\n",
    "        \n",
    "            bert_output = layer(bert_output, attention_mask=attention_mask)[0]\n",
    "\n",
    "        token_reps = torch.cat([bert_output,outputs],dim=-1)\n",
    "        \n",
    "        lstm_outs, _ = self.lstm(token_reps)\n",
    "        \n",
    "        lstm_outs=self.slot_classifier(lstm_outs)\n",
    "        \n",
    "        return lstm_outs    \n",
    "    def calculate_loss(self,token_ids, attention_masks, token_mask, segment_ids, label_ids,\n",
    "                                                      label_masks, feats ):\n",
    "        \n",
    "        out = self(token_ids, attention_masks, feats)\n",
    "        \n",
    "        slot_loss = self.crf(out, label_ids, mask=attention_masks.byte(), reduction='mean') * -1.\n",
    "        \n",
    "        return slot_loss,(out, label_ids)\n",
    "    \n",
    "    def predict(self, exampe):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e4b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertTop(2, feats)\n",
    "model = model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff82a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "best_score = -1\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 1e-4},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "num_train_optimization_steps = len(train_iterator) // 1 * 100\n",
    "warmup_steps = int(0.1 * num_train_optimization_steps)\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=3e-4,)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=num_train_optimization_steps)\n",
    "for e in range(8):\n",
    "    logger.info(\"=\"*30 + f\"Epoch {e}\" + \"=\"*30)\n",
    "    tr_loss = update_model_weights(model, train_iterator, optimizer, scheduler)\n",
    "    logger.info(f\"train Loss: {tr_loss}\")\n",
    "    eval_loss, iob_metric, metric = evaluate(model, eval_iterator, processor.labels)\n",
    "    logger.info(f\"eval Loss: {eval_loss}\")\n",
    "    logger.info(f\"F1-Score tag: {metric['macro avg']['f1-score']}\")\n",
    "    logger.info(f\"F1-Score IOB-tag: {iob_metric['macro avg']['f1-score']}\")\n",
    "    logger.info(f\"Metric:\")\n",
    "#     print(metric)\n",
    "    logger.info(f\"\\tO: {metric['1']['f1-score']}\")\n",
    "    logger.info(f\"\\tB-GENDER: {metric['2']['f1-score']} \")\n",
    "    logger.info(f\"\\tI-GENDER: {metric['3']['f1-score']}\")\n",
    "    logger.info(f\"\\tB-LOC: {metric['4']['f1-score']}\")\n",
    "    logger.info(f\"\\tI-LOC: {metric['5']['f1-score']}\")\n",
    "\n",
    "\n",
    "    if iob_metric['macro avg']['f1-score'] > best_score:\n",
    "        best_score = iob_metric['macro avg']['f1-score']\n",
    "        best_epoch = e\n",
    "        model_path = f\"checkpoints/vner_model.bin\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logger.info(f\"Model save at epoch {best_epoch} with best score {best_score}\")\n",
    "        summary_writer.add_text(\"Best result\", f\"F1-Score: {best_score}\", best_epoch)\n",
    "        summary_writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    " processor.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3556c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'em muốn đặt khám cho mẹ em thái tự mẫu'\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import NOTSET\n",
    "from torch import nn\n",
    "\n",
    "from model.layer import WordRepresentation, FeedforwardLayer, BiaffineLayer\n",
    "from transformers import AutoConfig\n",
    "import os\n",
    "from pathlib import Path\n",
    "from preprocess.static_features import Feature\n",
    "from model.layer.featureEmbed import FeatureRep\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from model.layer.crf import CRF\n",
    "from model.layer.utils import get_extended_attention_mask\n",
    "from underthesea import word_tokenize\n",
    "# text=word_tokenize(text, format=\"text\")\n",
    "class BertCRF(nn.Module):\n",
    "    def __init__(self, n_layer_bert = -1, cfg_feat=None, pretrained_bert='vinai/phobert-base'):\n",
    "        super().__init__()\n",
    "        if cfg_feat is None:\n",
    "            path = Path(__file__).parent.absolute()\n",
    "            path = Path(path).parent.absolute()\n",
    "            cfg_feat = os.path.join(path,\"resources/features/feature_config.json\")\n",
    "        feats=Feature(cfg_feat)\n",
    "        self.word_net  = FeatureRep(feats)\n",
    "        path = Path(__file__).parent.absolute()\n",
    "        path = Path(path).parent.absolute()\n",
    "        self.fe = FeatureExtractor(dict_dir=os.path.join(path,'resources/features'))\n",
    "        \n",
    "        b = AutoModel.from_pretrained(pretrained_bert).to(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "        #----------------\n",
    "        processor = NERProcessor(\"./dataset\", tokenizer)\n",
    "        processor.labels=[\"O\",'B-GENDER','I-GENDER','B-LOC','I-LOC']\n",
    "        processor.label_map= {label: i for i, label in enumerate(processor.labels, 1)}\n",
    "        self.processor=processor\n",
    "        #-----------------------------------\n",
    "        self.bert_embed =b.base_model.embeddings \n",
    "        \n",
    "        self.bert_layers = nn.ModuleList([b.base_model.encoder.layer[i] for i in range(top)])\n",
    "        \n",
    "        self.lstm = nn.LSTM(768 + 23, 768//2, batch_first=True, num_layers=2,bidirectional=True)\n",
    "        \n",
    "        self.crf = CRF(num_tags=6, batch_first=True)\n",
    "        \n",
    "        self.slot_classifier  = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(768, 6)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask,  feat_rep):\n",
    "        \n",
    "        bert_output = self.bert_embed(input_ids)\n",
    "        \n",
    "        attention_mask = get_extended_attention_mask(attention_mask,input_ids.shape,input_ids.device)\n",
    "        \n",
    "        outputs = self.word_net(feat_rep)\n",
    "\n",
    "        for layer in self.bert_layers:\n",
    "        \n",
    "            bert_output = layer(bert_output, attention_mask=attention_mask)[0]\n",
    "\n",
    "        token_reps = torch.cat([bert_output,outputs],dim=-1)\n",
    "        \n",
    "        lstm_outs, _ = self.lstm(token_reps)\n",
    "        \n",
    "        lstm_outs=self.slot_classifier(lstm_outs)\n",
    "        \n",
    "        return lstm_outs    \n",
    "    def calculate_loss(self,token_ids, attention_masks, token_mask, segment_ids, label_ids,\n",
    "                                                      label_masks, feats ):\n",
    "        \n",
    "        out = self(token_ids, attention_masks, feats)\n",
    "        \n",
    "        slot_loss = self.crf(out, label_ids, mask=attention_masks.byte(), reduction='mean') * -1.\n",
    "        \n",
    "        return slot_loss,(out, label_ids)\n",
    "    \n",
    "    def predict(self, sentence, device='cpu'):\n",
    "        sentence =word_tokenize(text, format=\"text\")\n",
    "#         fe = FeatureExtractor(dict_dir='resources/features')\n",
    "        feats_extracted=self.fe.extract_feature(text, ner_labels=None, is_segmentation=False)\n",
    "        fake_feats = [(0,feats_extracted[0], feats_extracted[1])]\n",
    "        features = self.processor.convert_sentences_to_features(fake_feats, 100, feats)\n",
    "        \n",
    "        token_ids = features[0].token_ids\n",
    "        attention_masks = features[0].attention_masks\n",
    "        feats = features[0].feats\n",
    "        \n",
    "        token_id_tensors = torch.tensor(token_ids, dtype=torch.long).to(device=device)\n",
    "        attention_mask_tensors = torch.tensor(attention_masks, dtype=torch.long).to(device=device)\n",
    "        \n",
    "        feat_tensors = {}\n",
    "        for feat_key, feat_value in feats.items():\n",
    "            feat_tensors[feat_key] = torch.tensor(feat_value, dtype=torch.long).to(device=device)\n",
    "        feat_tensors = {k:v[None,...] for k,v in feat_tensors.items()}\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(token_id_tensors[None,...], attention_mask_tensors[None,...], feat_tensors)\n",
    "            out1 = model.crf.decode(out)\n",
    "            out2 = torch.argmax(out,axis=-1)[0]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3ee29cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4fb22fcaadcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeats_extracted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_segmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fe' is not defined"
     ]
    }
   ],
   "source": [
    "feats_extracted=fe.extract_feature(text, ner_labels=None, is_segmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d46fa164",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_extracted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-edb4e6a5dcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats_extracted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m126\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_extracted' is not defined"
     ]
    }
   ],
   "source": [
    "features = processor.convert_examples_to_features([feats_extracted], 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ff60f65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_extracted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2665ae25f1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_extracted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_extracted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_extracted' is not defined"
     ]
    }
   ],
   "source": [
    "len(feats_extracted[0]),len(feats_extracted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b411a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feats_extracted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e9d1243ee380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_extracted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feats_extracted' is not defined"
     ]
    }
   ],
   "source": [
    "len(feats_extracted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c911d424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocess.processcer_join_bert.Example at 0x7f9546e60e20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29025989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 696.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cho anh nguyễn_văn nam xin thông_tin khám bệnh ở khoa cấp_cứu phía miền nam\n",
      "*** Example ***\n",
      "guid: 0\n",
      "tokens: cho anh nguy@@ ễn_@@ văn nam xin thông_tin khám bệnh ở khoa cấp_cứu phía miền nam\n",
      "input_ids: 3 13 83 27497 51220 2958 542 611 195 1621 326 25 2054 1629 216 460 542 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "attention_masks: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "valid_mask: 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "feats:\n",
      "\t[Case]: [1, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[SC]: [2, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[FW]: [1, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[QB]: [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[Num]: [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[LOC]: [1, 1, 2, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[ORG]: [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[PER]: [1, 2, 1, 2, 0, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t[PPOS]: [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'cho anh nguyễn văn nam xin thông tin khám bệnh ở khoa cấp cứu phía miền nam'\n",
    "from underthesea import word_tokenize\n",
    "text=word_tokenize(text, format=\"text\")\n",
    "# 'Chàng_trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'\n",
    "print(text)\n",
    "fe = FeatureExtractor(dict_dir='resources/features')\n",
    "feats_extracted=fe.extract_feature(text, ner_labels=None, is_segmentation=False)\n",
    "feats=Feature(\"/home/tuenguyen/Desktop/24mar2021/task_nlp/join_task_gender_department/resources/features/feature_config.json\")\n",
    "fake_feats = [(0,feats_extracted[0], feats_extracted[1])]\n",
    "features = processor.convert_sentences_to_features(fake_feats, 100, feats)\n",
    "dataset_test = NERdataset(features,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e5298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "339f8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, token_ids, attention_masks, token_mask, segment_ids, label_ids, label_masks, feats = dataset_test[0]\n",
    "feats = {k:v[None,...] for k,v in feats.items()}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(token_ids[None,...], attention_masks[None,...], feats)\n",
    "    out1 = model.crf.decode(out)\n",
    "    out2 = torch.argmax(out,axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c4a041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c418cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = [max(i-1,0) for i in out1[0]]\n",
    "# out = out - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd7d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-GENDER', 'B-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'I-LOC', 'I-GENDER', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER', 'B-LOC', 'I-LOC', 'I-GENDER', 'O', 'B-GENDER']\n",
      "['I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n"
     ]
    }
   ],
   "source": [
    "# processor.label_map\n",
    "print([processor.labels[i] for i in out1] )\n",
    "print([processor.labels[max(i-1,0)] for i in out2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slot_preds = np.array(model.crf.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([processor.labels[i] for i in out1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[132][0],train_data[132][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(eval_data[1000][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import chunk\n",
    "chunk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52439b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
